{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "U6ekd-APqLnD",
    "outputId": "6d837320-d55b-437b-a105-bf7f994f74ec"
   },
   "outputs": [],
   "source": [
    "# uploading backend requirements file to Colab\n",
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4yWCSafvqa_6",
    "outputId": "b9ebb506-f73e-428c-d1de-b646e62d0f69"
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6GZ0kRQFmPb4",
    "outputId": "de198819-b11f-4863-c7e8-9c7d9a891ded"
   },
   "outputs": [],
   "source": [
    "!pip install -q bitsandbytes accelerate transformers sentencepiece --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752,
     "referenced_widgets": [
      "5806299301474c3fbfffcef201d886f8",
      "75483e8c45d54b3ba4bfed01ae86549e",
      "83231ea14c064393b33b644bf7c1bee6",
      "77781c48bb304129997e5173bcf36e8b",
      "d37b1bdf064f4724a03a50b16f940124",
      "cd74a56c49e246d4afa2d6a23b28e2ae",
      "7aab8e9f3c624e9d8632649b97ecaa66",
      "d8d0d1d4d00c4417b15939b8e84528b7",
      "b69439a4ad2d413fbbd2fd1709494385",
      "c17e9e47b2cb4df2ba8d15f9892d0d56",
      "6ed32245094a46aaa16445159ee7d5e7",
      "e443812c3eb94e519e31eb97ef9aaa80",
      "4fe59ac931eb4a249000d2107ffe3b8a",
      "a68b66a93a804ed5907995e2376c7b09",
      "7e14b36f39ca47419da2f9c8bbef9e30",
      "ae7657bcbe8b43aaab198bbef4fecf4c",
      "e64e243d6a054cc18c57a2e51b83194c",
      "e9a1f1cf26534ebb9e518e7b75f6a698",
      "aeeaeab2e8824c8ba1eda13fe273a8e1",
      "ff90bdc5c90d4ff1baf0cd21b2cc3b09",
      "4f10b959b8d542308383d8b958ec8405",
      "b3ecd79a84d047b2b1ebcfccda3695fd",
      "a4608bd1aea64731aaf77fb189359b58",
      "a9609818ba0b476fb68297026b247a63",
      "a1bb6aa529474bd78cf2ea78a2cf70cf",
      "8848fea347ce463fbcf1cf1bd1ad34ab",
      "9a94a49c2adc42c2b4b38ac0bf53a216",
      "3497e1b913e44d90b9472fdcf4597b14",
      "b07f4d74f9a748f08fcc78c2abd28001",
      "13994289b0384eddb3d5d813e70763d4",
      "457e6ce1f3de4bddbc82b5b9099a2906",
      "61d181241ca3469dbbe1c26559d5c333",
      "277b42e147004d85adb9b1da31b3f1a0",
      "c042b7e9dff244c48ddee65e917c730f",
      "00318369646041bfba747c76086524e7",
      "fdaa068fa1364a7e834ef922fcfec05b",
      "fc74e7dad6a7462c835c3f78dffb6f3a",
      "d1a6c75897194ba38530e7656e367272",
      "2fd9d73e93fc4813afaa72d01247195c",
      "40e4f4b0214d49b6bed29b32d4321c88",
      "2b3dbb7353b54b519ce4a44ff28c3112",
      "cb2f314fde27416689a7bd398c311830",
      "590a40db52724f5a8ddee51fdaa8e4f4",
      "64cc1b663ed841368155c4a76273ff8c",
      "33fd4ce4bf51444b9c53ccfe406fb3a0",
      "f0278eb809484c53af25fe5d1f464abe",
      "9a543b62cb404ebd9ab362c755fecd57",
      "3927e2cd3a234fbf992aa81b728e7cd8",
      "ce05dab3c7c04aac8575125e7d2035ae",
      "0c2827a6aabd45a385defd0a452add3d",
      "b77a0b6adaae43a4a13b6f274b68c412",
      "2edf1c629e664d9fb2e5751dae07d422",
      "bc5c39e87825489fbd9ce372afa5cf8b",
      "613fb67e87c7421fab8e307684714cf9",
      "333f49b0a460447895fc88574517bce3",
      "46c40b338f7b44b899c069b16905ff26",
      "4d6143d107a04fe6ad94894e6c8f45c8",
      "c860162ecb614c6a882452cc34968187",
      "1a347a1b964e4275a431b537fd9b0c5b",
      "06f4bbf34c344a9d9c67a1cd5ada2c25",
      "b5655a5d2d0f444c96df8007c35ad523",
      "4ea153f1ef6c458bb2e183ef6830ac30",
      "0b84db686ba043f3b5fd246341ccda2a",
      "6f50223720b24f0da919f5d561c38af9",
      "0ed823ada90142cba6ba01841055e2a4",
      "9ca8dc5802f94dde9c89610aae6df698",
      "ac64a47073004d7b9a5cdb81d59c7b22",
      "fb4d45a9cdf54b189d694661f30b501d",
      "497b55f0a5aa4e428b35eab68719f6cb",
      "da70ba05ff3646b6aff1c420fe7193c6",
      "10a4bc8dcea34c21a7a0410784fabcc2",
      "894f357506854e37be71a54216e2f4fd",
      "c459bbd5797c4f69b7da27515f910dbd",
      "007d1728357140a789fd07efa22e2671",
      "59546e491f444ccf969eabe831fcf72f",
      "e44cb09c861548ec87f9a43be3792225",
      "8180cd63942846b889d313b936f9bf77",
      "67c2e6233d474ad08c30289fa9e42505",
      "589bd6f2b3cc4838ba69ada660ddc348",
      "be42009030fe417d9bda811b4a12c5f4",
      "97b7e6f6227846fc814bfef572edb905",
      "56421d37b574459893540a2f210d95ed",
      "ffb1c668807d411aa403615756d7f345",
      "0ad1701cd01442ac934d7ea23e0db71f",
      "8476f1d43a3442378b3409904839d133",
      "3b0ca5c9c1624d0a9c67f5a59326688f",
      "064c08654fc34106894796ccd25fd0e2",
      "10714ba9e750465c896b0b2aaa3f0fa1",
      "8a4b25e8c4b245828702eba1542e779a",
      "9bec2ea8761f453b8af59c297e1f37e1",
      "db7f86a025c94f0888736fe450c72358",
      "9c40271e74664528912e5616ddb544e6",
      "23992e21a46b4156ae3e7a485bfabaf0",
      "10aef40b25134841a214d9d276a13709",
      "1332defc0a11470d91a9bc033a213bee",
      "c42122d87b0445ff8e4b64592cb06e53",
      "60e53ff56961478b870f75aa0ea94052",
      "98e32788084149ed9c5d3191b4fc5f91",
      "0929323d7a7b458fbda2471155a53467",
      "61550189d2c5451d8bc7ecde91320710",
      "fbeee803b27b47988e44bc0064511459",
      "3ca654bc0bf2418fb29114c8a5b3efaa",
      "c6bdb062a9d1433795524d73e337d0e2",
      "2bfeff5e056846d8b05011f412c76c54",
      "cbc04e553b50424f96f9f10fb406d7a2",
      "fdeacac9719d4472a49caa46968a5bdd",
      "cd6488747cb946d79f1e0cda37cc3f9e",
      "d716179a2f864523aa4eb2006bac6508",
      "2b2e23b650b542c192de7e0f4390cbc2",
      "22c8827aee4e46cb84befbb5e0e6b982",
      "d57cc09bd2c44f91b6ec5da8c8ae0f59",
      "7b74ac0baa7c47b2bbe09063c8a255e3",
      "f64c2baee2a741e0882735e245787758",
      "a8bce6395ac94228a2eb4f0cfc091d63",
      "cb6e99e9719a424aa62b5562037b33dc",
      "93958d9d79ad486fa3cb6f9f3ea159e2",
      "62d655b903e542ae94d0c199e297e9e6",
      "73b7ad3463ca4218afb3559f9076e4ff",
      "8af2e62a3f8148808fb67c8f28624d96",
      "05fe7bded16d40ef84446eabccc7eec0",
      "7f2fb7dc72ad4c4c843cb39c00441ec6",
      "83e80cf15d3249f0aaef953ad088a886",
      "956cfae6d32841b68b454c4c3901b1c7",
      "907c503bb5ce456e8dae951c8e4c3f98",
      "874a5a88f38a4709b1b25bf03dbc8d9c",
      "bc8582d838ad4587b676d9615ebab447",
      "6450c04360de4785a72ac193122fca1b",
      "b1efd8c53337424e8e87e106d6867072",
      "8523e2bfee2e4b2986d58a0f6f8478f2",
      "dc9bf32ff14546afad3aa63a5cf01e01",
      "61a209190da34ba3b80f7e121926d077",
      "87f4053c1b5c440c9f7bc1fde42b88de",
      "48c970e95fd14996bf8f05a2b4cfe986",
      "33d977187a7c4a92a4899e4c29ecdc31",
      "1adbd53705f143e2ae4411c9d3cd2e37",
      "e21ef51e2c35417295daf4c6a8e41e03",
      "3f6178eb691940f78902008bd00a1872",
      "a6b03cdc8b8949bf87e95afe11416a08",
      "17301af390c54c3d8da90aa26c0c2ae2",
      "8b180c18e88d421e913edb40f4b193d4",
      "a704c664fb574163805ae6a737702b90",
      "e88218525c3e4a89b69784f0f26a575a",
      "50f5b444652d4c88ae3687206d341b6f",
      "e07ec65fa70945f2a412f29e39230a37",
      "6e17f3761aa9412dba0f6ff066f6d2d1",
      "817ab6384ebe4f08965ca9975ba05599",
      "2d30b8d0d65a4fc080727e2495ed74be",
      "d34cc0a4a28d4f7fb869277d9e4c8a4e",
      "b89e99a77ceb4820be634812985ccf00",
      "d696c16895dd4cc78fc48fd1c9f785a7",
      "fc5161c0d51c4548bbe4eb35ef8a481e",
      "51c14c108d8747779f291eb9d4b6df96",
      "6a016fe1a1f74f11903ba91239510c15",
      "6a7fb182c03e4d03b05ae9697f4f9230",
      "79af0f44cb77454782509febe7dd1f9a",
      "5fe2b8881a7940b89b28f5f06dbecef5",
      "4a02fc2909a64b919d3da71c27fdadbe",
      "028237ebdb84454eb1c5887debcc15f8",
      "6584e1c609d447d39b4f6e6bb56a3f4b",
      "526da5209fca46b7943bea838398556d",
      "e37498321b2246aaa0e4e1367ee407fd",
      "1a2b7bfa8e25447bb325a54a7e578bfb",
      "2ffdf31184ad44f3a4193fe149f3ac41",
      "523c367c19e24f8b8b7eb0b52e446b81",
      "d634cc5a8eac49ca80108f77cb9c7b16",
      "ac947ac38bff46f19dd3a2341b137c6e",
      "89cc5ccbc65349f3ad9a37425c696f30",
      "18181de40f024d92ba78c437799d742d",
      "8fa3fc2395734b62a056e24a51131d8c",
      "92ee98a890914a5eac1168ccd03fbaef",
      "b8ef3e699afd40a9abc6543279a1de22",
      "770b69b383c64a0c9f1f247ff3e410df",
      "fe93438d4ba74dd0ba431d80c89ae45b",
      "353bb6953c6f45529396c2f9e42287bd",
      "ed99211595014a4e966d07a608cb5f46",
      "99a6ef722f024befadf049caa23181b2",
      "c24c0e7c124e4e30a865fd92a70f5d33",
      "aa2debd1b1b746229cbf07671cfa7882",
      "71677e2715314f7b95db130b1313759b",
      "be2f544534c64a1f897a239b53fdfd7f",
      "9abe8fd04a9e4db49a8f451b47d09380",
      "79589302a4924edb9ea5de37bdbf9d86",
      "ab4f7a2af64a466094df8dd71c71d68d",
      "26b7c2473f2a400d88fa0302b4e9962a",
      "88d44565516e40149cc156e86b2d7643",
      "5cb9bd5ce5bc43f98bf933f969844f71",
      "887d6d7206b1476c906297934a0a2fe5",
      "efbecc8f2cc946738319635ab4596006",
      "d9cb78b0bc674897955e182d2d303ba2",
      "d66da2aea88641b986dbf1a43490a1c2",
      "f8822c7f54724ef7ad76b066f1e4ec9d",
      "87ee2c1348cb4c27bdad9b1e7201e46f",
      "89d04015048f46918cfb26ac537188d1",
      "daaea9ba72474faba2f04547f0eec2b0",
      "248b98d644df465abedbef6cdd4d2054",
      "a704628d2f58499e9c794fcb8621c9f6",
      "3e46100686d74a8a9e294c8544385e93",
      "ab7f6c0a879a4ba596b8104b3e0b9cf7"
     ]
    },
    "id": "I2NkwDnzmWe3",
    "outputId": "faf9b21a-fbac-42a3-ad7d-e956c8617a64"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# 4-bit quantization for Zephyr-7B-β\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Zephyr-7B-β loaded — ready for recipes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fgjylYdmcZf"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_recipe(ingredients):\n",
    "    ings = \", \".join(ingredients)\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a world-class creative chef. Generate ONE beautiful, detailed recipe using ONLY these ingredients: {ings}\n",
    "Include:\n",
    "- A catchy, creative title\n",
    "- Full ingredients with realistic quantities\n",
    "- 5–8 numbered steps with clear instructions\n",
    "- Cooking time and servings\n",
    "Do NOT repeat. Do NOT add extra ingredients.</|system|>\n",
    "<|user|>\n",
    "Generate the recipe now.</|user|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.split(\"<|assistant|>\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 157
    },
    "id": "VMMIMG-DnepZ",
    "outputId": "7842b9ff-391b-47f3-cb93-64621cd573a9"
   },
   "outputs": [],
   "source": [
    "generate_recipe(['chicken', 'garlic', 'lemon', 'bellpepper', 'potato'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848,
     "referenced_widgets": [
      "a34253d27dd643b684995877409237b0",
      "e68ac6a189af42f7b49862d2e6d86b16",
      "b352ba7d12174c17a41445c86325979d",
      "af19e1ca90f846ee81bc9ad97ad53f97",
      "592e91327a63404aac17f4b0af84b06a",
      "a309a73818814c64a6b5f0d9fbf3f964",
      "1ade8ae543fd4a29b5527e1aebb01de2",
      "0a8686accdb3409fa11d246eb60a6f8d",
      "1e82d952aea544bbb41d812af71c4565",
      "cc439e68bdbb4c199de251feb894cf00",
      "42c10061a9eb48cc9921dc9aef3348db"
     ]
    },
    "id": "oxv3vSRLp9Uz",
    "outputId": "20d9ef82-abae-46c1-ec92-d50e0bfa0f45"
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from pyngrok import ngrok, conf\n",
    "import uvicorn\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# ngrok token \n",
    "conf.get_default().auth_token = \"Your_Ngrok_Auth_Token_Here\"\n",
    "\n",
    "# FastAPI app setup\n",
    "app = FastAPI(title=\"AI Chef Pro — Zephyr-7B\")\n",
    "# CORS is needed for frontend to connect from localhost\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "print(\"Loading Zephyr-7B-β in 4-bit (takes ~90 seconds on first run)...\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "MODEL_ID = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=False  \n",
    ")\n",
    "\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"ZEPHYR-7B-β LOADED\")\n",
    "\n",
    "# Pydantic models for request/response\n",
    "class RecipeRequest(BaseModel):\n",
    "    ingredients: list[str]\n",
    "\n",
    "class RecipeResponse(BaseModel):\n",
    "    recipe: str\n",
    "    status: str = \"success\"\n",
    "\n",
    "def generate_recipe(ingredients: list[str]) -> str:\n",
    "    ings = \", \".join(ingredients)\n",
    "\n",
    "    prompt = f\"\"\"<|system|>\n",
    "You are a world-renowned creative chef. Create ONE stunning, restaurant-quality recipe using ONLY these ingredients: {ings}\n",
    "\n",
    "Requirements:\n",
    "- Give a catchy, creative title\n",
    "- Include servings and total cooking time\n",
    "- List ingredients with realistic quantities\n",
    "- Write 5–8 clear, detailed numbered steps\n",
    "- Optional: add a plating suggestion\n",
    "- NEVER repeat yourself\n",
    "- NEVER add ingredients not listed\n",
    "</|system|>\n",
    "<|user|>\n",
    "Generate the recipe now.\n",
    "</|user|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=600,  \n",
    "        temperature=0.75,  \n",
    "        top_p=0.90,\n",
    "        repetition_penalty=1.25,  \n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # extract only the recipe part, ignore the prompt\n",
    "    recipe_text = full_response.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "    return recipe_text\n",
    "\n",
    "# API Endpoints\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"AI Chef Pro with Zephyr-7B is running!\", \"model\": \"Zephyr-7B-β\"}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"healthy\", \"model\": \"Zephyr-7B-β\"}\n",
    "\n",
    "@app.post(\"/api/generate-recipe\", response_model=RecipeResponse)\n",
    "async def create_recipe(request: RecipeRequest):\n",
    "    # basic validation\n",
    "    if not request.ingredients or len(request.ingredients) == 0:\n",
    "        raise HTTPException(status_code=400, detail=\"Please provide at least one ingredient\")\n",
    "\n",
    "    recipe = generate_recipe(request.ingredients)\n",
    "    return RecipeResponse(recipe=recipe)\n",
    "\n",
    "# Start server + ngrok\n",
    "if __name__ == \"__main__\":\n",
    "    PORT = 8000\n",
    "\n",
    "    print(\"\\n\" + \"═\"*85)\n",
    "    print(\"STARTING AI CHEF PRO WITH ZEPHYR-7B-β\")\n",
    "    print(\"═\"*85)\n",
    "\n",
    "    # create ngrok tunnel so frontend can access from local machine\n",
    "    tunnel = ngrok.connect(PORT)\n",
    "    public_url = f\"{tunnel.public_url}/api/generate-recipe\"\n",
    "\n",
    "    print(f\"\\nPUBLIC API URL → {public_url}\")\n",
    "    print(\"═\"*85)\n",
    "\n",
    "    # auto-copy URL to clipboard \n",
    "    try:\n",
    "        from google.colab import output\n",
    "        output.eval_js(f'navigator.clipboard.writeText(\"{public_url}\")')\n",
    "        print(\"URL AUTO-COPIED TO CLIPBOARD! Just press Ctrl+V in your frontend!\")\n",
    "    except:\n",
    "        print(\"Copy the URL above manually\")\n",
    "\n",
    "    print(\"═\"*85)\n",
    "    print(\"AI CHEF PRO IS LIVE — TIME TO COOK!\")\n",
    "    print(\"═\"*85)\n",
    "\n",
    "    # run the server\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=PORT, log_level=\"info\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
